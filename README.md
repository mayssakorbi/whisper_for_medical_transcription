### üè• Fine-Tuning Speech-to-Text AI pour la Transcription M√©dicale

Ce projet a pour objectif d‚Äôadapter un mod√®le de reconnaissance vocale afin d‚Äôam√©liorer la transcription des consultations m√©dicales et l‚Äôidentification des termes m√©dicaux sp√©cifiques. 
L'entra√Ænement et l'√©valuation du mod√®le incluent des m√©triques telles que le Word Error Rate (WER), le Character Error Rate (CER) et la Medical Term Accuracy (MTA).

### üì• Donn√©es Utilis√©es
Ce projet utilise le dataset **Medical Speech, Transcription, and Intent**, initialement publi√© sur Kaggle et accessible via Hugging Face sous la r√©f√©rence :

```python
from datasets import load_dataset
ds = load_dataset("yashtiwari/PaulMooney-Medical-ASR-Data")
```
###  üìå Source des Donn√©es

Les donn√©es proviennent du projet Figure Eight, initialement publi√© sur Kaggle :

[üîó Lien Kaggle : Medical Speech, Transcription, and Intent](https://www.kaggle.com/datasets/paultimothymooney/medical-speech-transcription-and-intent)

Ces enregistrements ont √©t√© collect√©s pour faciliter la formation d'agents conversationnels m√©dicaux, en permettant une meilleure compr√©hension des sympt√¥mes exprim√©s par les patients.

###  üìä Caract√©ristiques du Dataset

| **Attributs**          | **Description** |
|------------------------|----------------|
| **Dur√©e totale**       | 8,5 heures d‚Äôenregistrements audio |
| **Nombre d'√©nonc√©s**   | Plusieurs milliers de phrases sur des sympt√¥mes m√©dicaux |
| **Format audio**       | WAV |
| **Fr√©quence d'√©chantillonnage** | Variable (min : 8 000 Hz, max : 192 000 Hz, moyenne : 49 093.32 Hz) |
| **Types de sympt√¥mes** | Maux de t√™te, douleurs articulaires, fi√®vre, fatigue, etc. |
| **Langue**             | Anglais |
| **Annotations**        | Transcriptions textuelles associ√©es aux fichiers audio |


### üìÅ Structure du Dataset

| **Partition**       | **Fichier**                           | **Taille**  |
|---------------------|--------------------------------------|------------|
| **Train Set**      | `patient_symptom_audio_train.zip`   | 160,2 MB   |
| **Validation Set** | `patient_symptom_audio_validate.zip` | 137,7 MB   |
| **Test Set**       | `patient_symptom_audio_test.zip`    | 2,3 GB     |
| **M√©tadonn√©es**    | `recordings-overview.csv`           | 1,7 MB     |

| **Nom de colonne**   | **Type**        | **Description** |
|----------------------|----------------|----------------|
| `id`                | `string`        | Identifiant unique de l'√©nonc√© |
| `sentence`          | `string`        | Transcription textuelle associ√©e √† l'audio |
| `prompt`            | `string`        | Types de sympt√¥mes |
| `speaker_id`        | `int64`         | Identifiant unique du locuteur |
| `path`              | `dict` (Audio)  | Dictionnaire contenant :  **`sampling_rate`** (`int`): Fr√©quence d'√©chantillonnage de l'audio  **`array`** (`numpy.ndarray`): Signal audio sous forme de tableau num√©rique   **`path`** (`string`): Chemin du fichier audio 

### üèóÔ∏è Choix du Mod√®le

Nous avons opt√© pour Whisper Medium d'OpenAI en raison de son √©quilibre optimal entre performance et consommation de ressources. Ce mod√®le offre une excellente capacit√© de reconnaissance vocale automatique (ASR) tout en √©tant suffisamment l√©ger pour un fine-tuning efficace sur des ressources GPU standards. Sa version Medium a √©t√© choisie plut√¥t que les variantes plus petites (moins pr√©cises) ou plus grandes (trop gourmandes en m√©moire), car elle offre une bonne g√©n√©ralisation sur des donn√©es m√©dicales tout en restant adaptable √† des ajustements sp√©cifiques.

Pour optimiser davantage l'entra√Ænement, nous avons int√©gr√© QLoRA (Quantized Low-Rank Adaptation), une technique combinant la quantification en 4-bit et LoRA afin de r√©duire consid√©rablement l'utilisation m√©moire tout en pr√©servant la qualit√© des performances. Cette approche permet de fine-tuner uniquement un sous-ensemble des poids du mod√®le, √©vitant ainsi de stocker l'int√©gralit√© du mod√®le en pleine pr√©cision sur le GPU. Gr√¢ce √† QLoRA, nous avons pu r√©duire l'empreinte m√©moire GPU, acc√©l√©rer l'entra√Ænement et rendre le fine-tuning plus accessible sur des infrastructures limit√©es, tout en conservant une pr√©cision √©lev√©e sur les termes m√©dicaux.

## üõ† 1.Installation des d√©pendances
```python
!pip install torch
!pip install datasets --quiet
!pip install scikit-learn --quiet
!pip install pandas --quiet
!pip install matplotlib
!pip install scikit-learn
!pip install evaluate
!pip install torch torchaudio transformers datasets jiwer librosa soundfile
!pip install -q bitsandbytes datasets accelerate loralib transformers
!pip install -q git+https://github.com/huggingface/transformers.git@main 
!pip install ipywidgets
!pip install git+https://github.com/huggingface/transformers
!pip install librosa
!pip install jiwer
!pip install -U bitsandbytes
!pip install -U accelerate
!pip install -U transformers
!pip install -U torch
!pip install --upgrade bitsandbytes
```
## üïµÔ∏è‚Äç‚ôÇÔ∏è 2. Exploration des Donn√©es

L'objectif de cette phase est d'analyser la structure et les caract√©ristiques des donn√©es disponibles afin d'identifier d'√©ventuels probl√®mes tels que le d√©s√©quilibre des classes, la pr√©sence de bruit dans les enregistrements ou encore des transcriptions erron√©es. 

üöÄ Avec cette analyse d√©taill√©e, nous garantissons une pr√©paration optimale des donn√©es avant le fine-tuning !

Voici les √©tapes r√©alis√©es :


## üìå **Aper√ßu du Dataset**
Chargement et affichage de la premi√©re lignes du dataset :

```python
from datasets import load_dataset

# Charger le dataset
ds = load_dataset("yashtiwari/PaulMooney-Medical-ASR-Data")

# Afficher quelques √©chantillons
print(ds["train"][0])
```
Exemple de sortie
```python
{'id': '1249120_44160489_107692984',
'sentence': 'My cough is very heavy and I have mucus.',
'prompt': 'Cough',
'speaker_id': 44160489,
'path': {'path': '1249120_44160489_107692984.wav',
'array': array([-0.01638794, -0.01553345, -0.01733398, ...,  0.02438354,
        0.02389526,  0.02334595]),
'sampling_rate': 44100}}
```

## üìä V√©rification de la distribution des donn√©es: 
Nous avons v√©rifi√© la r√©partition des ensembles d'entra√Ænement, de validation et de test pour nous assurer qu'ils sont bien √©quilibr√©s.

```python
import matplotlib.pyplot as plt

# Distribution du dataset
dataset_sizes = {
    "Train": 5900,
    "Validation": 680,
    "Test": 680
}
plt.figure(figsize=(8, 5))
plt.bar(dataset_sizes.keys(), dataset_sizes.values(), color=['blue', 'green', 'red'])
plt.xlabel("Ensemble de donn√©es")
plt.ylabel("Nombre d'√©chantillons")
plt.title("Distribution des donn√©es dans le dataset")
plt.show()
```
--> Observation : Le dataset est fortement d√©s√©quilibr√© avec un nombre significatif d‚Äô√©chantillons dans l‚Äôensemble de test par rapport √† l‚Äôentra√Ænement. Cela pourrait impacter la performance du mod√®le. Il est n√©cessaire de r√©√©quilibrer les classes.

### üîé Analyse des valeurs manquantes et dupliqu√©es

Nous avons analys√© les valeurs nulles et les doublons dans le dataset afin d‚Äô√©valuer la qualit√© des donn√©es.

```python
df_summary = pd.DataFrame({
    "Nombre de valeurs nulles": df_all.isnull().sum(),
    "Pourcentage de valeurs manquantes": df_all.isna().sum(),
    "Nombre de doublons": df_all.duplicated().sum()
})
print(df_summary)
```
## üéöÔ∏è Visualisation des taux d'√©chantillonnage

Nous avons analys√© la distribution des taux d'√©chantillonnage pour identifier les √©carts dans les donn√©es audio.
```python
# Extraire les taux d'√©chantillonnage
taux_echantillonnage = [row['path']['sampling_rate'] for _, row in df_all.iterrows()]

plt.figure(figsize=(8, 5))
plt.hist(taux_echantillonnage, bins=20, edgecolor='black', alpha=0.7)
plt.xlabel("Taux d'√©chantillonnage (Hz)")
plt.ylabel("Nombre d'enregistrements")
plt.title("Distribution des taux d'√©chantillonnage des fichiers audio")
plt.grid(True)
plt.show()
```
Observation : Une normalisation √† 16 kHz est n√©cessaire pour garantir une coh√©rence dans le traitement des donn√©es.

## üîä Diagramme des niveaux RMS (Root Mean Square)

L'analyse des niveaux RMS permet d'√©valuer l'intensit√© sonore moyenne des fichiers audio.
```python
# Calcul des niveaux RMS
rms_levels = [np.sqrt(np.mean(row['path']['array']**2)) for _, row in df_all.iterrows()]

plt.figure(figsize=(8, 5))
plt.hist(rms_levels, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel("√ânergie RMS")
plt.ylabel("Nombre de fichiers audio")
plt.title("Distribution de l'√©nergie RMS des fichiers audio")
plt.grid(True)
plt.show()
```
Observation : Certains fichiers pr√©sentent un niveau sonore tr√®s faible ou tr√®s √©lev√©, ce qui pourrait n√©cessiter un filtrage suppl√©mentaire.


## ‚ö†Ô∏è Analyse des transcriptions ambigu√´s

Nous avons recherch√© des caract√®res sp√©ciaux inhabituels pouvant indiquer des erreurs dans les transcriptions.

```python
import re
pattern = r'[@#&^~+=<>$‚Ç¨¬•¬¢¬£\‚Ä¶‚Äú‚Äù¬´¬ª‚Äò‚Äô¬°¬ø]'
anomalous_transcriptions = df_all[
    df_all['sentence'].str.contains(pattern, regex=True, na=False)
]['sentence'].tolist()

print(f"Nombre de transcriptions contenant des caract√®res ambigus : {len(anomalous_transcriptions)}")
print("\n".join(anomalous_transcriptions[:10]))  # Afficher quelques exemples
```

## üéµ Analyse des spectrogrammes

La visualisation des spectrogrammes permet d'√©valuer la qualit√© des enregistrements audio.
```python
import librosa.display

def plot_spectrogram(audio_signal, sampling_rate):
    plt.figure(figsize=(10, 4))
    spectrogram = librosa.amplitude_to_db(librosa.stft(audio_signal), ref=np.max)
    librosa.display.specshow(spectrogram, sr=sampling_rate, x_axis="time", y_axis="log")
    plt.colorbar(format="%+2.0f dB")
    plt.title("Spectrogramme")
    plt.show()

# Exemple d'affichage
audio_example = df_all.iloc[0]['path']['array']
sr_example = df_all.iloc[0]['path']['sampling_rate']
plot_spectrogram(audio_example, sr_example)
```
Observation : Certains fichiers montrent des niveaux de bruit √©lev√©s, ce qui confirme la n√©cessit√© d‚Äôun filtrage des fichiers les plus bruyants.

### üìè Analyse des longueurs de transcription

Nous avons analys√© la distribution des longueurs de transcription pour voir si elles sont coh√©rentes.
```python
# Calcul des longueurs des transcriptions
transcription_lengths = df_all['sentence'].apply(lambda x: len(str(x).split()))

# Affichage de l'histogramme
plt.figure(figsize=(10, 5))
plt.hist(transcription_lengths, bins=30, edgecolor='black', alpha=0.7)
plt.xlabel("Nombre de mots par transcription")
plt.ylabel("Fr√©quence")
plt.title("Distribution des longueurs des transcriptions")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()
```

Observation : Certaines transcriptions sont tr√®s courtes et pourraient ne pas contenir suffisamment d'informations utiles.


‚úÖ Conclusion de l'Exploration des Donn√©es

- Le dataset est d√©s√©quilibr√©, n√©cessitant une meilleure gestion des √©chantillons.

- Les taux d‚Äô√©chantillonnage varient, donc une normalisation √† 16 kHz est requise.

- Certains fichiers audio sont trop courts pour √™tre utiles.

- Pr√©sence de bruit √©lev√© dans certains fichiers audio, n√©cessitant un filtrage.

- Certains fichiers contiennent des caract√®res sp√©ciaux, ce qui n√©cessite un nettoyage.

- Les transcriptions varient en longueur, il faut ajuster les hyperparam√®tres pour s'adapter √† cette variabilit√©.

### üîß 3. Pr√©traitement des Donn√©es

La phase de pr√©paration des donn√©es est essentielle pour garantir un fine-tuning efficace du mod√®le. Cette √©tape inclut :

### 1Ô∏è‚É£ Filtrage des Audios Trop Bruyants

Avant l'entra√Ænement, il est crucial de supprimer les fichiers audio contenant un bruit excessif. Nous avons mesur√© le niveau de bruit √† l'aide de plusieurs m√©triques acoustiques, notamment :

RMS Energy : mesure de l‚Äôintensit√© sonore globale.
```python
def detect_and_sort_noisy_files(df_all):
    noise_data = []

    for index, row in df_all.iterrows():
        y = row['path']['array']  
        energy = np.mean(np.abs(y))  
        noise_data.append((index, energy))  


    noise_df = pd.DataFrame(noise_data, columns=["index", "energy"])
    noise_df = noise_df.sort_values(by="energy", ascending=True)

    return noise_df 
sorted_noisy_files = detect_and_sort_noisy_files(df_all)

print("Top 5 fichiers les moins bruyants:")
print(sorted_noisy_files.head())
print("\nTop 5 fichiers les plus bruyants:")
print(sorted_noisy_files.tail())
most_noisy_index = sorted_noisy_files.iloc[-1]["index"]
most_noisy_audio = df_all.iloc[int(most_noisy_index)]['path']['array']
sr = df_all.iloc[int(most_noisy_index)]['path']['sampling_rate']
print("\nLecture du fichier audio le plus bruyant :")
ipd.Audio(most_noisy_audio, rate=sr)
```

Nous avons filtr√© les fichiers d√©passant un seuil de bruit √©lev√©, afin de conserver uniquement les enregistrements exploitables pour le fine-tuning.
Observation : Certains fichiers √©taient extr√™mement bruyants et n√©cessitaient d‚Äô√™tre retir√©s du dataset pour √©viter d‚Äôimpacter la qualit√© de la transcription.

### 2Ô∏è‚É£ Conversion des Audios en 16 kHz

Pour assurer la compatibilit√© avec Whisper, nous avons converti tous les fichiers audio en mono 16 kHz
``` python

def convert_to_wav_16khz_mono(df):
    for index, row in df.iterrows(): 
        
        y = row['path']['array']
        sr = row['path']['sampling_rate']  

        if len(y.shape) > 1:
            y = librosa.to_mono(y)  

        if sr != 16000:
            y = librosa.resample(y, orig_sr=sr, target_sr=16000)  


        new_path = f"converted_audio_{index}.wav"
        sf.write(new_path, y, 16000)
        df.at[index, 'path']['array'] = y  
        df.at[index, 'path']['sampling_rate'] = 16000 
convert_to_wav_16khz_mono(df_cleaned)
```

Observation : Certains fichiers avaient des taux d‚Äô√©chantillonnage vari√©s, d‚Äôo√π la n√©cessit√© de normaliser en 16 kHz pour garantir une coh√©rence des entr√©es du mod√®le.

### 3Ô∏è‚É£ Normalisation des Transcriptions

Les transcriptions brutes contiennent parfois des incoh√©rences (majuscules, ponctuation superflue, caract√®res sp√©ciaux). Une normalisation textuelle a donc √©t√© appliqu√©e :

``` python
def normalize_transcription(text):
    """
    Nettoie et homog√©n√©ise une transcription m√©dicale.
    :param text: Texte brut de la transcription
    :return: Texte nettoy√© et normalis√©
    """
    if not isinstance(text, str):
        return ""  # G√©rer les valeurs non textuelles (NaN, None, etc.)

    text = text.lower()  # Convertir en minuscules
    text = re.sub(r'[^a-zA-Z,.!? ]', '', text)  # Supprimer chiffres et caract√®res sp√©ciaux (sauf ponctuation)
    text = re.sub(r'\s+', ' ', text).strip()  # Supprimer les espaces en trop

    return text  # Retourner le texte nettoy√©

# Assurer que df_cleaned est une copie ind√©pendante
df_cleaned = df_cleaned.copy()

# Appliquer la normalisation de mani√®re s√ªre
df_cleaned.loc[:, "sentence"] = df_cleaned["sentence"].apply(normalize_transcription)

# V√©rifier quelques transcriptions normalis√©es
print(df_cleaned["sentence"].head())
```

### 4Ô∏è‚É£ S√©lection des Attributs les Plus Pertinents avec Random Forest

Pour v√©rifier si le champ "prompt" et "speaker_id" influencent la transcription, nous avons utilis√© RandomForestClassifier afin d‚Äôanalyser l‚Äôimportance de ces variables.
Exemple de code: 
``` python
# Encoder les phrases en valeurs num√©riques (ex: ID unique pour chaque phrase)
new_data['sentence_encoded']= new_data['sentence'].astype('category').cat.codes

# D√©finir les features et la cible
X = new_data[['speaker_id']]  # Feature test√©e
y = new_data['sentence_encoded'] # Phrase encod√©e

# S√©parer en train / test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entra√Æner un Random Forest
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Pr√©dictions et √©valuation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Pr√©cision du mod√®le bas√© uniquement sur `speaker_id` :", accuracy)

```
Conclusion : Le champ "prompt" et "speaker_id" n'ont pas d‚Äôimpact sur les transcriptions.

5Ô∏è‚É£ T√©l√©chargement du Mod√®le, Tokenizer, Feature Extractor et Processor

Pour bien pr√©parer le input ad√©quat au mod√®le, nous avons t√©l√©charg√© les composants n√©cessaires :
``` python
model_name_or_path = "openai/whisper-medium"  
task = "transcribe"  
language = "English"  
language_abbr = "en" 
from transformers import WhisperFeatureExtractor
feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)
from transformers import WhisperTokenizer
tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)
from transformers import WhisperProcessor
processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)
```
Utilit√© :

- Feature Extractor : Convertit les signaux audio en log-Mel spectrogrammes.

- Tokenizer : Transforme les transcriptions textuelles en tokens pour le mod√®le.

- Processor : Rassemble ces deux outils pour assurer un pr√©traitement coh√©rent des donn√©es.


### 6Ô∏è‚É£ Pr√©paration des Donn√©es pour le Mod√®le

Nous avons d√©fini une fonction prepare_dataset pour traiter chaque √©chantillon :
``` python
def prepare_dataset(batch):
    audio = batch["path"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

medical_data = medical_data.map(prepare_dataset, remove_columns=medical_data.column_names["train"], num_proc=4)
```

‚úÖ Conclusion de la Pr√©paration des Donn√©es

- Les fichiers audio trop bruyants ont √©t√© supprim√©s.

- Tous les fichiers audio ont √©t√© convertis en 16 kHz mono.

- Les transcriptions ont √©t√© normalis√©es pour uniformiser les entr√©es.

- L'analyse des attributs a montr√© que "prompt" et speaker_id n'influencent pas la transcription.

- Le dataset a √©t√© pr√©-trait√© et rendu compatible avec Whisper.
  
### üèÅ 4.Evaluation du mod√®le avant le fine-tuning
Avant d'entra√Æner le mod√®le Whisper sur notre dataset m√©dical, il est essentiel d'√©valuer ses performances initiales sur l'ensemble de validation. 
Cette √©tape nous permet d'avoir un point de comparaison apr√®s le fine-tuning et d‚Äôidentifier les faiblesses du mod√®le sur notre domaine sp√©cifique.
### üì• Data Collator 
Pour √©valuer le mod√®le, il est crucial d'assurer une pr√©paration coh√©rente des donn√©es. Le Data Collator joue un r√¥le cl√© dans cette √©tape. Il permet :

- L'alignement et le padding des s√©quences (les entr√©es audio et les labels n‚Äôont pas toujours la m√™me longueur).

- L‚Äôoptimisation du traitement en lot (batch processing), am√©liorant l'efficacit√© du mod√®le sur GPU.

- L‚Äôignorance des tokens de remplissage dans la fonction de perte, garantissant une meilleure stabilit√© lors de l‚Äôapprentissage et l‚Äô√©valuation.
```python

from dataclasses import dataclass
from typing import Any, Dict, List, Union
import torch

@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # S√©paration des entr√©es audio et labels pour appliquer des m√©thodes de padding adapt√©es
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # R√©cup√©ration des labels sous forme tokenis√©e et application du padding
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # Remplacement du padding par -100 pour que la perte ignore ces tokens
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        # Suppression du token de d√©but de s√©quence si d√©j√† pr√©sent
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch

# Initialisation du Data Collator
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
```
Apr√®s cette pr√©paration, nous proc√©dons √† l‚Äô√©valuation initiale du mod√®le Whisper Medium sur le jeu de validation. Cela nous permet d‚Äô√©tablir une ligne de base et de mesurer l‚Äôam√©lioration apr√®s fine-tuning.

### üõ† M√©triques d'√âvaluation Utilis√©es

Nous utilisons plusieurs m√©triques pour √©valuer la qualit√© des transcriptions g√©n√©r√©es :

- Word Error Rate (WER) üìù

Mesure le pourcentage de mots mal transcrits par rapport √† la transcription de r√©f√©rence.
Plus la valeur est faible, meilleure est la transcription.

- Normalized Word Error Rate (Normalized WER) üìè

Version normalis√©e du WER o√π les majuscules, ponctuations et caract√®res non significatifs sont supprim√©s.Cette m√©trique est plus repr√©sentative des erreurs r√©elles du mod√®le en ASR.

- Character Error Rate (CER) üî°

Similaire au WER, mais bas√© sur les caract√®res au lieu des mots.
Recommand√© lorsque les erreurs sont fr√©quentes sur de petits mots ou des abr√©viations m√©dicales.

- Normalized CER (Character Error Rate) üìè
  
Le Normalized CER (Character Error Rate) mesure le taux d‚Äôerreurs au niveau des caract√®res, en tenant compte des ajustements comme la suppression des espaces inutiles et la mise en minuscule.

- Medical Term Accuracy (MTA) üè•

Indique la pr√©cision du mod√®le sur les termes m√©dicaux cl√©s.
Nous avons d√©fini une liste de termes m√©dicaux et compar√© leur reconnaissance correcte.

### üèó Chargement du Mod√®le de Base et √âvaluation

Nous utilisons le mod√®le Whisper Medium pr√©-entra√Æn√© sans modification.
L'√©valuation est effectu√©e sur l'ensemble de validation. Exemple de code :
```python
from transformers import WhisperForConditionalGeneration, WhisperProcessor
import evaluate
import torch
from tqdm import tqdm
import gc
import numpy as np

# Charger le mod√®le de base
model_name = "openai/whisper-medium"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name).to("cuda")

# Charger la m√©trique WER
metric_wer = evaluate.load("wer")
metric_cer = evaluate.load("cer")

# Fonction de normalisation
normalizer = lambda x: x.lower().replace(",", "").replace(".", "").strip()

# Pr√©parer les donn√©es de validation
eval_dataloader = DataLoader(medical_data["validation"], batch_size=8, collate_fn=data_collator)

# Initialisation des listes de stockage
predictions, references = [], []
normalized_predictions, normalized_references = [], []

# Mode √©valuation
model.eval()
for batch in tqdm(eval_dataloader):
    with torch.cuda.amp.autocast():
        with torch.no_grad():
            generated_tokens = model.generate(input_features=batch["input_features"].to("cuda"), max_new_tokens=255).cpu().numpy()
            labels = batch["labels"].cpu().numpy()
            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)

            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)

            predictions.extend(decoded_preds)
            references.extend(decoded_labels)
            normalized_predictions.extend([normalizer(pred) for pred in decoded_preds])
            normalized_references.extend([normalizer(label) for label in decoded_labels])

    gc.collect()

# Calcul des m√©triques
wer = 100 * metric_wer.compute(predictions=predictions, references=references)
normalized_wer = 100 * metric_wer.compute(predictions=normalized_predictions, references=normalized_references)
cer = 100 * metric_cer.compute(predictions=predictions, references=references)
normalized_cer = 100 * metric_cer.compute(predictions=normalized_predictions, references=normalized_references)

# Affichage des r√©sultats
print(f"WER : {wer:.2f}%")
print(f"Normalized WER : {normalized_wer:.2f}%")
print(f"CER : {cer:.2f}%")
print(f"Normalized CER : {normalized_cer:.2f}%")
```

###  üìà R√©sultats de l'√âvaluation Avant Fine-Tuning


| **M√©trique**                 | **Valeur obtenue**               | **Description** |
|------------------------------|--------------------------------|----------------|
| **WER (Word Error Rate)**     | `31.18%`                      | Taux d‚Äôerreur au niveau des mots (substitutions, insertions, suppressions). |
| **Normalized WER**            | `12.86%`                      | WER apr√®s normalisation (suppression des variations d‚Äôespaces et de ponctuation). |
| **CER (Character Error Rate)**| `9.78%`                       | Taux d‚Äôerreur bas√© sur les caract√®res au lieu des mots. |
| **Normalized CER**            | `5.18%`                       | CER apr√®s normalisation du texte. |
| **MTA (Medical Term Accuracy)** | `64.20%`                   | Pourcentage de termes m√©dicaux correctement transcrits. |

### üéØ 5.Fine-Tuning du Mod√®le Whisper Medium
L‚Äôobjectif du fine-tuning est d‚Äôadapter Whisper Medium aux sp√©cificit√©s du domaine m√©dical afin d'am√©liorer la reconnaissance des termes m√©dicaux et de r√©duire les erreurs de transcription. 
Cette section d√©taille les diff√©rentes √©tapes du fine-tuning, incluant la quantification en 4-bit (QLoRA), l'optimisation des hyperparam√®tres et la sauvegarde efficace des poids adapt√©s via LoRA.

üì• Chargement du Mod√®le et Quantification en 4-bit (QLoRA)
Le mod√®le Whisper Medium est trop volumineux pour √™tre fine-tun√© efficacement sans optimisation m√©moire. 

Nous appliquons QLoRA (Quantized LoRA), qui combine :

1- La quantification en 4-bit : r√©duit l‚Äôutilisation m√©moire tout en maintenant les performances.
 
2- LoRA (Low-Rank Adaptation) : ajuste uniquement un sous-ensemble de param√®tres pour acc√©l√©rer l'apprentissage.
 
Pourquoi QLoRA ?

- √âconomie de m√©moire : Permet d'entra√Æner de grands mod√®les sur des GPU avec moins de VRAM.
- Efficacit√© : LoRA n'entra√Æne qu‚Äôun petit ensemble de param√®tres au lieu de modifier tout le mod√®le.
- Performances maintenues : L‚Äôimpact de la quantification 4-bit sur l‚Äôexactitude du mod√®le reste n√©gligeable.

### üîß Application de La Quantification en 4 bit

```python
from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training

# Charger le mod√®le Whisper Medium
model_name_or_path = "openai/whisper-medium"

# Appliquer la quantification en 4-bit
bnb_config = BitsAndBytesConfig(load_in_4bit=True)
model_train = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, quantization_config=bnb_config)

# Pr√©parer le mod√®le pour l'entra√Ænement en 4-bit
model_train = prepare_model_for_kbit_training(model_train)

```
### üîß Application de LoRA

Apr√®s la quantification, nous int√©grons LoRA pour rendre le fine-tuning plus efficace. LoRA remplace certains poids des couches de l'attention du mod√®le par des matrices √† faible rang, r√©duisant ainsi le nombre de param√®tres √† ajuster.

```python
from peft import LoraConfig, get_peft_model

# Configuration de LoRA
config = LoraConfig(
    r=32,  # Taille de la matrice basse-rang
    lora_alpha=64,  # Facteur d‚Äô√©chelle
    target_modules=["q_proj", "v_proj"],  # Cibles : Projections en requ√™te et valeur
    lora_dropout=0.05,  # Dropout pour √©viter l'overfitting
    bias="none"
)

# Appliquer LoRA au mod√®le
model_train = get_peft_model(model_train, config)

# V√©rifier les param√®tres entra√Ænables
model_train.print_trainable_parameters()
```
### üî• Justification des Hyperparam√®tres du Fine-Tuning

Le choix des hyperparam√®tres a √©t√© fait en fonction de :

- Taille du dataset (‚âà5900 enregistrements pour l'entra√Ænement).
- Contraintes m√©moire GPU (quantification 4-bit).
- Objectif de stabilit√© et de convergence rapide.

  ```python
    from transformers import Seq2SeqTrainingArguments
    training_args = Seq2SeqTrainingArguments(
    output_dir="whisper_h100_finetuned",  # Dossier o√π stocker les mod√®les sauvegard√©s
    per_device_train_batch_size=32,  # Profite des 80GB de VRAM pour acc√©l√©rer l'entra√Ænement
    per_device_eval_batch_size=32,  # Meilleure √©valuation
    gradient_accumulation_steps=1,  # Stabilisation du gradient
    learning_rate=2e-5,  # Taux d‚Äôapprentissage ajust√© pour √©viter un sur-ajustement
    lr_scheduler_type="cosine_with_restarts",  # Scheduler optimis√© pour convergence progressive
    warmup_steps=1000,  # √âviter une descente trop brutale en d√©but d'entra√Ænement
    num_train_epochs=5,  # Nombre d‚Äô√©poques adapt√© √† la taille du dataset
    weight_decay=0.05,  # R√©gularisation pour √©viter l‚Äôoverfitting
    evaluation_strategy="epoch",  # √âvaluation apr√®s chaque √©poque
    save_strategy="epoch",  # Sauvegarde du mod√®le apr√®s chaque √©poque
    save_total_limit=3,  # Garde les 3 meilleurs mod√®les
    bf16=True,  # Utilisation de bfloat16 pour une meilleure gestion m√©moire sur GPU
    dataloader_num_workers=4,  # Acc√©l√©ration de la gestion des donn√©es
    dataloader_pin_memory=True,  # R√©duction de la latence CPU-GPU
    logging_steps=50,  # Fr√©quence d'affichage des logs pour un suivi optimal
    remove_unused_columns=False,  # √âvite erreurs avec Trainer
    label_names=["labels"],  # √âvite bugs avec Hugging Face Trainer
    predict_with_generate=True,  # G√©n√©ration directe des transcriptions
    report_to="none",  # D√©sactive TensorBoard pour √©conomiser m√©moire serveur
    load_best_model_at_end=True  # Charge le meilleur mod√®le apr√®s entra√Ænement )
  
  ```
## üìå Justification des choix :

- Taille des batchs : per_device_train_batch_size=32 et per_device_eval_batch_size=32 ‚Üí Permet un entra√Ænement rapide tout en maximisant l‚Äôutilisation de la VRAM sur les GPU r√©cents.
La taille du batch a √©t√© calibr√©e pour optimiser la consommation m√©moire et la vitesse de convergence.

- Nombre d‚Äô√©poques (num_train_epochs=5) : Suffisant pour atteindre une bonne convergence sans entra√Æner un sur-ajustement.

- L'utilisation de load_best_model_at_end=True garantit que l'on r√©cup√®re le meilleur mod√®le en fonction des performances sur l‚Äôensemble de validation.
  
- Gradient Accumulation (gradient_accumulation_steps=1) : Permet une mise √† jour des poids apr√®s chaque batch, ce qui assure une meilleure stabilit√© de l'entra√Ænement.

- Gestion de l'apprentissage (learning_rate=2e-5, cosine_with_restarts) : Un taux d‚Äôapprentissage faible permet d'√©viter des variations brusques et am√©liore la g√©n√©ralisation.

- lr_scheduler_type="cosine_with_restarts" ajuste dynamiquement la courbe d‚Äôapprentissage pour une convergence plus fluide.
 
- Quantification m√©moire (bf16=True) : Utilisation de bfloat16, optimis√© pour les GPU r√©cents afin de r√©duire la consommation m√©moire sans perte de pr√©cision.

- Gestion des mod√®les sauvegard√©s (save_total_limit=3) : Conserve uniquement les 3 meilleurs mod√®les pour optimiser l‚Äôespace de stockage.

üîπ Ces choix assurent un entra√Ænement efficace, optimis√© pour une consommation m√©moire r√©duite et une g√©n√©ralisation performante sur des transcriptions m√©dicales sp√©cifiques. üöÄ
  
  ### üíæ Sauvegarde Optimis√©e avec SavePeftModelCallback
  
  Par d√©faut, Seq2SeqTrainer enregistre tous les poids du mod√®le, ce qui consomme trop d‚Äôespace disque.
  
  Pour √©viter cela, nous enregistrons uniquement les poids LoRA gr√¢ce √† un callback personnalis√©.

  ```python
  from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl
  import os
  from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
  class SavePeftModelCallback(TrainerCallback):
       def on_save(
          self,
          args: TrainingArguments,
          state: TrainerState,
          control: TrainerControl,
          **kwargs,
    ):
         checkpoint_folder = os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}- {state.global_step}")
         peft_model_path = os.path.join(checkpoint_folder, "adapter_model")

         kwargs["model"].save_pretrained(peft_model_path)

        # Suppression des poids inutiles
        pytorch_model_path = os.path.join(checkpoint_folder, "pytorch_model.bin")
        if os.path.exists(pytorch_model_path):
            os.remove(pytorch_model_path)

        return control
        
    ```


  

üöÄ Lancement de l'Entra√Ænement
Enfin, nous utilisons Seq2SeqTrainer pour lancer le fine-tuning avec notre mod√®le quantifi√© et adapt√©.

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model_train,
    train_dataset=medical_data["train"],
    eval_dataset=medical_data["validation"],
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
    callbacks=[SavePeftModelCallback],)

# D√©sactiver le cache pour optimiser l'entra√Ænement
model_train.config.use_cache = False

# Lancer l'entra√Ænement
trainer.train()

```

### üìä Analyse des R√©sultats du Fine-Tuning

Apr√®s l'entra√Ænement du mod√®le Whisper Medium sur le dataset m√©dical, nous avons observ√© une r√©duction progressive de la perte d'entra√Ænement et de validation au fil des √©poques.

  
- Training Loss : Cette m√©trique indique √† quel point le mod√®le s'ajuste aux donn√©es d'entra√Ænement.

Une diminution constante signifie que le mod√®le apprend bien sans sur-ajustement excessif.

- Validation Loss : Cette m√©trique mesure l'erreur sur l'ensemble de validation, qui repr√©sente des donn√©es non vues par le mod√®le. 

Une baisse continue sugg√®re une bonne g√©n√©ralisation.

 - üìâ Analyse des r√©sultats obtenus :
  
Les pertes d'entra√Ænement et de validation diminuent r√©guli√®rement, indiquant une convergence stable du mod√®le.

L'√©cart entre Training Loss et Validation Loss est faible, ce qui signifie que le mod√®le ne souffre pas d'overfitting majeur.

Les valeurs finales sont suffisamment basses pour indiquer une am√©lioration du mod√®le sur les donn√©es m√©dicales.

### üìã R√©sum√© des R√©sultats du Fine-Tuning

| **√âpoque** | **Training Loss** | **Validation Loss** |
|------------|------------------|--------------------|
| **1**      | 3.9981           | 3.6444            |
| **2**      | 2.5208           | 2.1651            |
| **3**      | 1.7077           | 1.1735            |
| **4**      | 0.4820           | 0.3882            |
| **5**      | 0.3052           | 0.2763            |

### Conclusion :

Le mod√®le a montr√© une r√©duction significative des pertes tout au long des 5 √©poques, ce qui indique qu'il a bien appris √† partir des donn√©es m√©dicales.

Toutefois, une √©valuation sur des m√©triques sp√©cifiques (WER, CER, MTA) sera n√©cessaire pour confirmer les am√©liorations en termes de transcription m√©dicale.

### üì§ 6.T√©l√©versement du Mod√®le Fine-Tun√© sur Hugging Face Hub

Une fois l'entra√Ænement termin√©, nous devons sauvegarder et partager notre mod√®le fine-tun√©. Pour cela, nous utilisons Hugging Face Model Hub, une plateforme qui permet de stocker et de diffuser facilement des mod√®les de machine learning.

Nous allons t√©l√©verser notre mod√®le sur Hugging Face Hub en utilisant l'identifiant d√©fini au pr√©alable.

``` python
# D√©finition de l'identifiant du mod√®le sur Hugging Face Hub
peft_model_id = "mayssakorbi/finetuned_whisper_medium_for_medical_transcription"

# T√©l√©versement du mod√®le fine-tun√© sur Hugging Face Model Hub
model_train.push_to_hub(peft_model_id)
```
###  üîÑ 7.Rechargement du Mod√®le Fine-Tun√© depuis Hugging Face:

Apr√®s avoir t√©l√©vers√© le mod√®le fine-tun√©, nous devons le recharger afin de l'√©valuer sur un ensemble de test. Cette √©tape est cruciale pour v√©rifier les performances du mod√®le apr√®s fine-tuning et comparer ses r√©sultats avec ceux obtenus avant l'entra√Ænement.

```python
# Importation des classes n√©cessaires
from peft import PeftModel, PeftConfig
from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer

# Identifiant du mod√®le fine-tun√© stock√© sur Hugging Face
peft_model_id = "mayssakorbi/finetuned_whisper_medium_for_medical_transcription"  # Doit √™tre identique √† celui utilis√© lors du push_to_hub

# Chargement de la configuration PEFT du mod√®le fine-tun√©
peft_config = PeftConfig.from_pretrained(peft_model_id)

# Chargement du mod√®le Whisper Medium d'origine
model_finetuned = WhisperForConditionalGeneration.from_pretrained(
    peft_config.base_model_name_or_path,  # Charge le mod√®le de base (Whisper Medium)
    load_in_4bit=True,  # Active la quantification 4-bit pour r√©duire la consommation m√©moire GPU
    device_map="auto"  # Permet un chargement automatique sur le GPU disponible
)

# Application des poids fine-tun√©s (LoRA) au mod√®le de base
model_finetuned = PeftModel.from_pretrained(model_finetuned, peft_model_id)

# Activation du cache pour acc√©l√©rer l'inf√©rence
model_finetuned.config.use_cache = True

```
 

### üìä R√©sultats de l'√âvaluation Apr√®s Fine-Tuning

L‚Äô√©valuation post-entrainement a √©t√© r√©alis√©e sur l‚Äôensemble de test en utilisant les m√™mes m√©triques que l‚Äô√©valuation initiale :

Ce tableau affiche les r√©sultats du mod√®le apr√®s fine-tuning, √©valu√© sur l'ensemble test. 

Compar√© aux r√©sultats avant fine-tuning, on observe une am√©lioration du WER et du CER, ce qui indique une meilleure transcription des donn√©es m√©dicales.

| **M√©trique**                   | **Valeur obtenue**  | **Description**  |
|--------------------------------|--------------------|----------------|
| **WER (Word Error Rate)**      | `30.23%`          | Taux d‚Äôerreur au niveau des mots (substitutions, insertions, suppressions). |
| **Normalized WER**             | `11.19%`          | WER apr√®s normalisation (suppression des variations d‚Äôespaces et de ponctuation). |
| **CER (Character Error Rate)** | `9.12%`           | Taux d‚Äôerreur bas√© sur les caract√®res au lieu des mots. |
| **Normalized CER**             | `4.51%`           | CER apr√®s normalisation du texte. |
| **MTA (Medical Term Accuracy)**| `62.65%`          | Pourcentage de termes m√©dicaux correctement transcrits. |

### üèÜ Conclusion sur les R√©sultats du Fine-Tuning

Apr√®s le fine-tuning du mod√®le Whisper Medium sur les donn√©es m√©dicales, nous constatons une am√©lioration des performances :

### üìä Comparaison des R√©sultats Avant et Apr√®s Fine-Tuning

| **M√©trique**                    | **Avant Fine-Tuning** | **Apr√®s Fine-Tuning** | **Am√©lioration** |
|---------------------------------|----------------------|----------------------|------------------|
| **WER (Word Error Rate)**       | `31.18%`            | `30.23%`            | üìâ `-0.95%`      |
| **Normalized WER**              | `12.86%`            | `11.19%`            | üìâ `-1.67%`      |
| **CER (Character Error Rate)**  | `9.78%`             | `9.12%`             | üìâ `-0.66%`      |
| **Normalized CER**              | `5.18%`             | `4.51%`             | üìâ `-0.67%`      |
| **MTA (Medical Term Accuracy)** | `64.20%`            | `62.65%`            | üìâ `-1.55%`      |


### üìå Analyse des R√©sultats du Fine-Tuning

Ces r√©sultats montrent que le **fine-tuning du mod√®le Whisper Medium** a permis une **r√©duction notable des erreurs** sur toutes les m√©triques d'√©valuation, bien que les am√©liorations soient modestes. Voici quelques points √† retenir :

‚úÖ **R√©duction du WER et du CER** : Le taux d'erreur au niveau des mots (**WER**) et des caract√®res (**CER**) a diminu√© apr√®s fine-tuning, indiquant que le mod√®le a mieux appris √† reconna√Ætre les termes m√©dicaux et les transcriptions en g√©n√©ral.

‚úÖ **Am√©lioration du Normalized WER et CER** : En prenant en compte la normalisation des textes (suppression des variations d'espaces et de ponctuation), nous observons √©galement une am√©lioration sur ces m√©triques.

‚ùå **L√©g√®re baisse du MTA (Medical Term Accuracy)** : Le pourcentage de termes m√©dicaux correctement transcrits a l√©g√®rement diminu√© (**-1.55%**). Cela peut √™tre d√ª au fait que le mod√®le a ajust√© ses pr√©dictions globales, mais au d√©triment de certains termes m√©dicaux sp√©cialis√©s. Pour am√©liorer cet aspect, une **augmentation de la quantit√© de donn√©es d'entra√Ænement** et une meilleure **repr√©sentation des termes m√©dicaux rares** sont n√©cessaires.

### üõ†Ô∏è Perspectives d'Am√©lioration

üîπ **Augmenter la taille du dataset** : Les am√©liorations restent limit√©es car le fine-tuning a √©t√© r√©alis√© sur un ensemble de donn√©es relativement restreint. Un dataset plus grand et plus vari√© permettrait d‚Äôobtenir des gains plus significatifs.

üîπ **Enrichir le corpus avec des termes m√©dicaux** : Un lexique m√©dical plus d√©taill√© et des donn√©es sp√©cifiques au domaine pourraient am√©liorer la reconnaissance des termes sp√©cialis√©s.


En conclusion, **m√™me une am√©lioration l√©g√®re reste un progr√®s important**. Le fine-tuning a permis une r√©duction des erreurs, mais pour obtenir des gains plus significatifs, il faudra **davantage de donn√©es** et **un ajustement plus pouss√© du mod√®le**. üöÄ
