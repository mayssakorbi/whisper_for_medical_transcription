### üè• Fine-Tuning Speech-to-Text AI pour la Transcription M√©dicale

Ce projet a pour objectif d‚Äôadapter un mod√®le de reconnaissance vocale afin d‚Äôam√©liorer la transcription des consultations m√©dicales et l‚Äôidentification des termes m√©dicaux sp√©cifiques. 
L'entra√Ænement et l'√©valuation du mod√®le incluent des m√©triques telles que le Word Error Rate (WER), le Character Error Rate (CER) et la Medical Term Accuracy (MTA).

### üì• Donn√©es Utilis√©es
Ce projet utilise le dataset **Medical Speech, Transcription, and Intent**, initialement publi√© sur Kaggle et accessible via Hugging Face sous la r√©f√©rence :

```python
from datasets import load_dataset
ds = load_dataset("yashtiwari/PaulMooney-Medical-ASR-Data")
```
###  üìå Source des Donn√©es

Les donn√©es proviennent du projet Figure Eight, initialement publi√© sur Kaggle :

[üîó Lien Kaggle : Medical Speech, Transcription, and Intent](https://www.kaggle.com/datasets/paultimothymooney/medical-speech-transcription-and-intent)

Ces enregistrements ont √©t√© collect√©s pour faciliter la formation d'agents conversationnels m√©dicaux, en permettant une meilleure compr√©hension des sympt√¥mes exprim√©s par les patients.

###  üìä Caract√©ristiques du Dataset

| **Attributs**          | **Description** |
|------------------------|----------------|
| **Dur√©e totale**       | 8,5 heures d‚Äôenregistrements audio |
| **Nombre d'√©nonc√©s**   | Plusieurs milliers de phrases sur des sympt√¥mes m√©dicaux |
| **Format audio**       | WAV |
| **Fr√©quence d'√©chantillonnage** | Variable (min : 8 000 Hz, max : 192 000 Hz, moyenne : 49 093.32 Hz) |
| **Types de sympt√¥mes** | Maux de t√™te, douleurs articulaires, fi√®vre, fatigue, etc. |
| **Langue**             | Anglais |
| **Annotations**        | Transcriptions textuelles associ√©es aux fichiers audio |


### üìÅ Structure du Dataset

| **Partition**       | **Fichier**                           | **Taille**  |
|---------------------|--------------------------------------|------------|
| **Train Set**      | `patient_symptom_audio_train.zip`   | 160,2 MB   |
| **Validation Set** | `patient_symptom_audio_validate.zip` | 137,7 MB   |
| **Test Set**       | `patient_symptom_audio_test.zip`    | 2,3 GB     |
| **M√©tadonn√©es**    | `recordings-overview.csv`           | 1,7 MB     |

| **Nom de colonne**   | **Type**        | **Description** |
|----------------------|----------------|----------------|
| `id`                | `string`        | Identifiant unique de l'√©nonc√© |
| `sentence`          | `string`        | Transcription textuelle associ√©e √† l'audio |
| `prompt`            | `string`        | Types de sympt√¥mes |
| `speaker_id`        | `int64`         | Identifiant unique du locuteur |
| `path`              | `dict` (Audio)  | Dictionnaire contenant :  **`sampling_rate`** (`int`): Fr√©quence d'√©chantillonnage de l'audio  **`array`** (`numpy.ndarray`): Signal audio sous forme de tableau num√©rique   **`path`** (`string`): Chemin du fichier audio 

### üèóÔ∏è Choix du Mod√®le

Nous avons opt√© pour Whisper Medium d'OpenAI en raison de son √©quilibre optimal entre performance et consommation de ressources. Ce mod√®le offre une excellente capacit√© de reconnaissance vocale automatique (ASR) tout en √©tant suffisamment l√©ger pour un fine-tuning efficace sur des ressources GPU standards. Sa version Medium a √©t√© choisie plut√¥t que les variantes plus petites (moins pr√©cises) ou plus grandes (trop gourmandes en m√©moire), car elle offre une bonne g√©n√©ralisation sur des donn√©es m√©dicales tout en restant adaptable √† des ajustements sp√©cifiques.

Pour optimiser davantage l'entra√Ænement, nous avons int√©gr√© QLoRA (Quantized Low-Rank Adaptation), une technique combinant la quantification en 4-bit et LoRA afin de r√©duire consid√©rablement l'utilisation m√©moire tout en pr√©servant la qualit√© des performances. Cette approche permet de fine-tuner uniquement un sous-ensemble des poids du mod√®le, √©vitant ainsi de stocker l'int√©gralit√© du mod√®le en pleine pr√©cision sur le GPU. Gr√¢ce √† QLoRA, nous avons pu r√©duire l'empreinte m√©moire GPU, acc√©l√©rer l'entra√Ænement et rendre le fine-tuning plus accessible sur des infrastructures limit√©es, tout en conservant une pr√©cision √©lev√©e sur les termes m√©dicaux.

## üõ† 1.Installation des d√©pendances
```python
!pip install torch
!pip install datasets --quiet
!pip install scikit-learn --quiet
!pip install pandas --quiet
!pip install matplotlib
!pip install scikit-learn
!pip install evaluate
!pip install torch torchaudio transformers datasets jiwer librosa soundfile
!pip install -q bitsandbytes datasets accelerate loralib transformers
!pip install -q git+https://github.com/huggingface/transformers.git@main 
!pip install ipywidgets
!pip install git+https://github.com/huggingface/transformers
!pip install librosa
!pip install jiwer
!pip install -U bitsandbytes
!pip install -U accelerate
!pip install -U transformers
!pip install -U torch
!pip install --upgrade bitsandbytes
```
## üïµÔ∏è‚Äç‚ôÇÔ∏è 2. Exploration des Donn√©es

L'objectif de cette phase est d'analyser la structure et les caract√©ristiques des donn√©es disponibles afin d'identifier d'√©ventuels probl√®mes tels que le d√©s√©quilibre des classes, la pr√©sence de bruit dans les enregistrements ou encore des transcriptions erron√©es. Voici les √©tapes r√©alis√©es :

üöÄ Avec cette analyse d√©taill√©e, nous garantissons une pr√©paration optimale des donn√©es avant le fine-tuning !

## üìå **Aper√ßu du Dataset**
Chargement et affichage de la premi√©re lignes du dataset :

```python
from datasets import load_dataset

# Charger le dataset
ds = load_dataset("yashtiwari/PaulMooney-Medical-ASR-Data")

# Afficher quelques √©chantillons
print(ds["train"][0])
```
Exemple de sortie
```python
{'id': '1249120_44160489_107692984',
'sentence': 'My cough is very heavy and I have mucus.',
'prompt': 'Cough',
'speaker_id': 44160489,
'path': {'path': '1249120_44160489_107692984.wav',
'array': array([-0.01638794, -0.01553345, -0.01733398, ...,  0.02438354,
        0.02389526,  0.02334595]),
'sampling_rate': 44100}}
```

## üìä V√©rification de la distribution des donn√©es: 
Nous avons v√©rifi√© la r√©partition des ensembles d'entra√Ænement, de validation et de test pour nous assurer qu'ils sont bien √©quilibr√©s.

```python
import matplotlib.pyplot as plt

# Distribution du dataset
dataset_sizes = {
    "Train": 5900,
    "Validation": 680,
    "Test": 680
}
plt.figure(figsize=(8, 5))
plt.bar(dataset_sizes.keys(), dataset_sizes.values(), color=['blue', 'green', 'red'])
plt.xlabel("Ensemble de donn√©es")
plt.ylabel("Nombre d'√©chantillons")
plt.title("Distribution des donn√©es dans le dataset")
plt.show()
```
--> Observation : Le dataset est fortement d√©s√©quilibr√© avec un nombre significatif d‚Äô√©chantillons dans l‚Äôensemble de test par rapport √† l‚Äôentra√Ænement. Cela pourrait impacter la performance du mod√®le. Il est n√©cessaire de r√©√©quilibrer les classes.

### üîé Analyse des valeurs manquantes et dupliqu√©es

Nous avons analys√© les valeurs nulles et les doublons dans le dataset afin d‚Äô√©valuer la qualit√© des donn√©es.

```python
df_summary = pd.DataFrame({
    "Nombre de valeurs nulles": df_all.isnull().sum(),
    "Pourcentage de valeurs manquantes": df_all.isna().sum(),
    "Nombre de doublons": df_all.duplicated().sum()
})
print(df_summary)
```
## üéöÔ∏è Visualisation des taux d'√©chantillonnage

Nous avons analys√© la distribution des taux d'√©chantillonnage pour identifier les √©carts dans les donn√©es audio.
```python
# Extraire les taux d'√©chantillonnage
taux_echantillonnage = [row['path']['sampling_rate'] for _, row in df_all.iterrows()]

plt.figure(figsize=(8, 5))
plt.hist(taux_echantillonnage, bins=20, edgecolor='black', alpha=0.7)
plt.xlabel("Taux d'√©chantillonnage (Hz)")
plt.ylabel("Nombre d'enregistrements")
plt.title("Distribution des taux d'√©chantillonnage des fichiers audio")
plt.grid(True)
plt.show()
```
Observation : Une normalisation √† 16 kHz est n√©cessaire pour garantir une coh√©rence dans le traitement des donn√©es.

## üîä Diagramme des niveaux RMS (Root Mean Square)

L'analyse des niveaux RMS permet d'√©valuer l'intensit√© sonore moyenne des fichiers audio.
```python
# Calcul des niveaux RMS
rms_levels = [np.sqrt(np.mean(row['path']['array']**2)) for _, row in df_all.iterrows()]

plt.figure(figsize=(8, 5))
plt.hist(rms_levels, bins=50, edgecolor='black', alpha=0.7)
plt.xlabel("√ânergie RMS")
plt.ylabel("Nombre de fichiers audio")
plt.title("Distribution de l'√©nergie RMS des fichiers audio")
plt.grid(True)
plt.show()
```
Observation : Certains fichiers pr√©sentent un niveau sonore tr√®s faible ou tr√®s √©lev√©, ce qui pourrait n√©cessiter un filtrage suppl√©mentaire.


## ‚ö†Ô∏è Analyse des transcriptions ambigu√´s

Nous avons recherch√© des caract√®res sp√©ciaux inhabituels pouvant indiquer des erreurs dans les transcriptions.

```python
import re
pattern = r'[@#&^~+=<>$‚Ç¨¬•¬¢¬£\‚Ä¶‚Äú‚Äù¬´¬ª‚Äò‚Äô¬°¬ø]'
anomalous_transcriptions = df_all[
    df_all['sentence'].str.contains(pattern, regex=True, na=False)
]['sentence'].tolist()

print(f"Nombre de transcriptions contenant des caract√®res ambigus : {len(anomalous_transcriptions)}")
print("\n".join(anomalous_transcriptions[:10]))  # Afficher quelques exemples
```

Analyse du volume des enregistrements pour identifier d‚Äô√©ventuelles irr√©gularit√©s.

## üéµ Analyse des spectrogrammes

La visualisation des spectrogrammes permet d'√©valuer la qualit√© des enregistrements audio.
```python
import librosa.display

def plot_spectrogram(audio_signal, sampling_rate):
    plt.figure(figsize=(10, 4))
    spectrogram = librosa.amplitude_to_db(librosa.stft(audio_signal), ref=np.max)
    librosa.display.specshow(spectrogram, sr=sampling_rate, x_axis="time", y_axis="log")
    plt.colorbar(format="%+2.0f dB")
    plt.title("Spectrogramme")
    plt.show()

# Exemple d'affichage
audio_example = df_all.iloc[0]['path']['array']
sr_example = df_all.iloc[0]['path']['sampling_rate']
plot_spectrogram(audio_example, sr_example)
```
Observation : Certains fichiers montrent des niveaux de bruit √©lev√©s, ce qui confirme la n√©cessit√© d‚Äôun filtrage des fichiers les plus bruyants.

### üìè Analyse des longueurs de transcription

Nous avons analys√© la distribution des longueurs de transcription pour voir si elles sont coh√©rentes.
```python
# Calcul des longueurs des transcriptions
transcription_lengths = df_all['sentence'].apply(lambda x: len(str(x).split()))

# Affichage de l'histogramme
plt.figure(figsize=(10, 5))
plt.hist(transcription_lengths, bins=30, edgecolor='black', alpha=0.7)
plt.xlabel("Nombre de mots par transcription")
plt.ylabel("Fr√©quence")
plt.title("Distribution des longueurs des transcriptions")
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()
```

Observation : Certaines transcriptions sont tr√®s courtes et pourraient ne pas contenir suffisamment d'informations utiles.


‚úÖ Conclusion de l'Exploration des Donn√©es

- Le dataset est d√©s√©quilibr√©, n√©cessitant une meilleure gestion des √©chantillons.

- Les taux d‚Äô√©chantillonnage varient, donc une normalisation √† 16 kHz est requise.

- Certains fichiers audio sont trop courts pour √™tre utiles.

- Pr√©sence de bruit √©lev√© dans certains fichiers audio, n√©cessitant un filtrage.

- Certains fichiers contiennent des caract√®res sp√©ciaux, ce qui n√©cessite un nettoyage.

- Les transcriptions varient en longueur, il faut ajuster les hyperparam√®tres pour s'adapter √† cette variabilit√©.

### üîß 3.Pr√©traitement des Donn√©es

La phase de pr√©paration des donn√©es est essentielle pour garantir un fine-tuning efficace du mod√®le. Cette √©tape inclut :

### 1Ô∏è‚É£ Filtrage des Audios Trop Bruyants

Avant l'entra√Ænement, il est crucial de supprimer les fichiers audio contenant un bruit excessif. Nous avons mesur√© le niveau de bruit √† l'aide de plusieurs m√©triques acoustiques, notamment :

RMS Energy : mesure de l‚Äôintensit√© sonore globale.
```python
def detect_and_sort_noisy_files(df_all):
    noise_data = []

    for index, row in df_all.iterrows():
        y = row['path']['array']  
        energy = np.mean(np.abs(y))  
        noise_data.append((index, energy))  


    noise_df = pd.DataFrame(noise_data, columns=["index", "energy"])
    noise_df = noise_df.sort_values(by="energy", ascending=True)

    return noise_df 
sorted_noisy_files = detect_and_sort_noisy_files(df_all)

print("Top 5 fichiers les moins bruyants:")
print(sorted_noisy_files.head())
print("\nTop 5 fichiers les plus bruyants:")
print(sorted_noisy_files.tail())
most_noisy_index = sorted_noisy_files.iloc[-1]["index"]
most_noisy_audio = df_all.iloc[int(most_noisy_index)]['path']['array']
sr = df_all.iloc[int(most_noisy_index)]['path']['sampling_rate']
print("\nLecture du fichier audio le plus bruyant :")
ipd.Audio(most_noisy_audio, rate=sr)
```

Nous avons filtr√© les fichiers d√©passant un seuil de bruit √©lev√©, afin de conserver uniquement les enregistrements exploitables pour le fine-tuning.
Observation : Certains fichiers √©taient extr√™mement bruyants et n√©cessitaient d‚Äô√™tre retir√©s du dataset pour √©viter d‚Äôimpacter la qualit√© de la transcription.

### 2Ô∏è‚É£ Conversion des Audios en 16 kHz

Pour assurer la compatibilit√© avec Whisper, nous avons converti tous les fichiers audio en mono 16 kHz
``` python

def convert_to_wav_16khz_mono(df):
    for index, row in df.iterrows(): 
        
        y = row['path']['array']
        sr = row['path']['sampling_rate']  

        if len(y.shape) > 1:
            y = librosa.to_mono(y)  

        if sr != 16000:
            y = librosa.resample(y, orig_sr=sr, target_sr=16000)  


        new_path = f"converted_audio_{index}.wav"
        sf.write(new_path, y, 16000)
        df.at[index, 'path']['array'] = y  
        df.at[index, 'path']['sampling_rate'] = 16000 
convert_to_wav_16khz_mono(df_cleaned)
```

Observation : Certains fichiers avaient des taux d‚Äô√©chantillonnage vari√©s, d‚Äôo√π la n√©cessit√© de normaliser en 16 kHz pour garantir une coh√©rence des entr√©es du mod√®le.

### 3Ô∏è‚É£ Normalisation des Transcriptions

Les transcriptions brutes contiennent parfois des incoh√©rences (majuscules, ponctuation superflue, caract√®res sp√©ciaux). Une normalisation textuelle a donc √©t√© appliqu√©e :

``` python
def normalize_transcription(text):
    """
    Nettoie et homog√©n√©ise une transcription m√©dicale.
    :param text: Texte brut de la transcription
    :return: Texte nettoy√© et normalis√©
    """
    if not isinstance(text, str):
        return ""  # G√©rer les valeurs non textuelles (NaN, None, etc.)

    text = text.lower()  # Convertir en minuscules
    text = re.sub(r'[^a-zA-Z,.!? ]', '', text)  # Supprimer chiffres et caract√®res sp√©ciaux (sauf ponctuation)
    text = re.sub(r'\s+', ' ', text).strip()  # Supprimer les espaces en trop

    return text  # Retourner le texte nettoy√©

# Assurer que df_cleaned est une copie ind√©pendante
df_cleaned = df_cleaned.copy()

# Appliquer la normalisation de mani√®re s√ªre
df_cleaned.loc[:, "sentence"] = df_cleaned["sentence"].apply(normalize_transcription)

# V√©rifier quelques transcriptions normalis√©es
print(df_cleaned["sentence"].head())
```

### 4Ô∏è‚É£ S√©lection des Attributs les Plus Pertinents avec Random Forest

Pour v√©rifier si le champ "prompt" et "speaker_id" influencent la transcription, nous avons utilis√© RandomForestClassifier afin d‚Äôanalyser l‚Äôimportance de ces variables.
Exemple de code: 
``` python
# Encoder les phrases en valeurs num√©riques (ex: ID unique pour chaque phrase)
new_data['sentence_encoded']= new_data['sentence'].astype('category').cat.codes

# D√©finir les features et la cible
X = new_data[['speaker_id']]  # Feature test√©e
y = new_data['sentence_encoded'] # Phrase encod√©e

# S√©parer en train / test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entra√Æner un Random Forest
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Pr√©dictions et √©valuation
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Pr√©cision du mod√®le bas√© uniquement sur `speaker_id` :", accuracy)

```
Observation : Le champ "prompt" et "speaker_id" n'ont pas d‚Äôimpact sur les transcriptions.

5Ô∏è‚É£ T√©l√©chargement du Mod√®le, Tokenizer, Feature Extractor et Processor

Avant le fine-tuning, nous avons t√©l√©charg√© les composants n√©cessaires :
``` python
model_name_or_path = "openai/whisper-medium"  
task = "transcribe"  
language = "English"  
language_abbr = "en" 
from transformers import WhisperFeatureExtractor
feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)
from transformers import WhisperTokenizer
tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)
from transformers import WhisperProcessor
processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)
```
Utilit√© :

- Feature Extractor : Convertit les signaux audio en log-Mel spectrogrammes.

- Tokenizer : Transforme les transcriptions textuelles en tokens pour le mod√®le.

- Processor : Rassemble ces deux outils pour assurer un pr√©traitement coh√©rent des donn√©es.


### 6Ô∏è‚É£ Pr√©paration des Donn√©es pour le Mod√®le

Nous avons d√©fini une fonction prepare_dataset pour traiter chaque √©chantillon :
``` python
def prepare_dataset(batch):
    audio = batch["path"]
    batch["input_features"] = processor.feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"]
    ).input_features[0]
    batch["labels"] = processor.tokenizer(batch["sentence"]).input_ids
    return batch

medical_data = medical_data.map(prepare_dataset, remove_columns=medical_data.column_names["train"], num_proc=4)
```

‚úÖ Conclusion de la Pr√©paration des Donn√©es

- Les fichiers audio trop bruyants ont √©t√© supprim√©s.

- Tous les fichiers audio ont √©t√© convertis en 16 kHz mono.

- Les transcriptions ont √©t√© normalis√©es pour uniformiser les entr√©es.

- L'analyse des attributs a montr√© que "prompt" influence la transcription.

- Le dataset a √©t√© pr√©-trait√© et rendu compatible avec Whisper.
- 
### üèÅ 4.Plan pour l‚Äô√©valuation avant le fine-tuning
Avant d'entra√Æner le mod√®le Whisper sur notre dataset m√©dical, il est essentiel d'√©valuer ses performances initiales sur l'ensemble de validation. Cette √©tape nous permet d'avoir un point de comparaison apr√®s le fine-tuning et d‚Äôidentifier les faiblesses du mod√®le sur notre domaine sp√©cifique.

